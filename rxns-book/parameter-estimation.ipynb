{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Parameter estimation and rate law determination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The rate constants in rate laws generally must be determined from experiments\n",
    "\n",
    "-   We typically fit models to experimental data, and derive the rate constants from the fitted parameters\n",
    "\n",
    "-   There are two general types of fitting\n",
    "    -   linear regression\n",
    "        -   fitting models that are linear in the parameters\n",
    "    \n",
    "    -   nonlinear regression\n",
    "        -   fitting models that are nonlinear in the parameters\n",
    "\n",
    "-   In either case we want to estimate the value of the parameters in the model, and the uncertainty in the parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Linear regression review\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In linear regression we fit a model that is linear in the parameters to some data.\n",
    "\n",
    "-   The model parameters may be directly useful, e.g. the slope of a line may be related to a rate constant, or we may use the model to derive something, e.g. the derivative at some value.\n",
    "\n",
    "-   A linear model is one like:\n",
    "\n",
    "\\begin{equation}\n",
    "y = p_0 + p_1 f_1(x) + p_2 f_2(x) + \\cdots\n",
    "\\end{equation}\n",
    "\n",
    "-   Here the parameters are $p_i$ and the model is linear in them.\n",
    "-   The functions $f_i(x)$ do not have to be linear\n",
    "-   Some examples are:\n",
    "    -   $ y = p_0 + p_1 x $ - a line\n",
    "    -   $ y = p_0 + p_1 x + p_2 x^2 $ - a parabola\n",
    "    -   $ y = p_0 + p_1 e^x $\n",
    "\n",
    "-   We will write these in the general matrix algebra form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bf{y} = \\bf{X} \\bf{p}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bf{y} = \\left [\n",
    "\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\\\\\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\bf{X} = \\left [ \\begin{array}{cccc}\n",
    "f_n(x_1) & \\cdots & f_1(x_1) & 1 \\\\\n",
    "f_n(x_2) & \\cdots & f_1(x_2) & 1 \\\\\n",
    "\\vdots & \\vdots   & \\vdots   & \\vdots \\\\\n",
    "f_n(x_n) & \\cdots & f_1(x_n) & 1 \\\\\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\end{equation}\n",
    "\n",
    "and $ \\bf{p} = \\left [\\begin{array}{c}p_n \\\\ p_{n-1} \\\\ \\vdots \\\\ p_0 \\end{array} \\right ]  $\n",
    "\n",
    "-   The model will usually not fit data perfectly, so we modify the model to include the errors\n",
    "\n",
    "\\begin{equation}\n",
    "\\bf{y} = \\bf{X} \\bf{p} + \\bf{e}\n",
    "\\end{equation}\n",
    "\n",
    "-   We want the best estimate for **p**, which means we want the  **p** that minimizes the error in the least squares sense (that is the magnitude of the sum of squared errors is minimized)\n",
    "\n",
    "-   The best estimate for **p** is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bf{p} = (\\bf{X}^T\\bf{X})^{-1}\\bf{X}^T \\bf{y}\n",
    "\\end{equation}\n",
    "\n",
    "-   There will typically be errors between the data and model with corresponding uncertainty in the estimated parameters\n",
    "\n",
    "-   We need to quantify the uncertainty to determine how important it is in reactor design\n",
    "\n",
    "Let us consider an example. We want to fit a line to the following data. The line has an equation $y = p_0 x + p_1$.\n",
    "\n",
    "-   Remember\n",
    "\n",
    "\\begin{equation}\n",
    "\\bf{p} = (\\bf{X}^T\\bf{X})^{-1}\\bf{X}^T \\bf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Here we solve a prototypical problem of fitting a line to some data. We are given some x and y data, and we want to fit a line.\n",
    "\n",
    "[numpy.column_stack](https://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html)\n",
    "\n",
    "-   You should always plot the fitted function over the data to visually assess the quality of the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slope is -0.3145221843003413 \n",
      "and intercept is 0.000624573378839699\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8daf678240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([0, 0.5, 1, 1.5, 2.0, 3.0, 4.0, 6.0, 10])\n",
    "y = np.array([0, -0.157, -0.315, -0.472, -0.629, -0.942, -1.255, -1.884, -3.147])\n",
    "\n",
    "X = np.column_stack([x, x**0])\n",
    "\n",
    "#  I find these intermediate variables make it easier to read\n",
    "XTX = np.dot(X.T, X)\n",
    "XTy = np.dot(X.T, y)\n",
    "\n",
    "p = np.dot(np.linalg.inv(XTX), XTy)\n",
    "slope, intercept = p # note the order in X\n",
    "print('The slope is {0} \\nand intercept is {1}'.format(slope, intercept))\n",
    "\n",
    "# plot data and fit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, np.dot(X, p), 'r--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('images/la-line-fit.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo the last example by defining a function to calculate the summed squared error between the model and data, and use fmin to minimize the summed squared error. Show that you get the same parameters.\n",
    "\n",
    "-   The error in the fit is defined as: $\\bf{e} = \\bf{y} - \\bf{X}\\cdot \\bf{p}$\n",
    "\n",
    "-   We can compute the summed squared error as $SSE = \\bf{e} \\cdot \\bf{e}$\n",
    "-   We define $SST = \\sum (\\bf{y} - \\overline{y})^2 = (\\bf{y} - \\overline{y})\\cdot(\\bf{y} - \\overline{y})$\n",
    "\n",
    "-   We can use that to compute $R^2 = 1 - SSE/SST$ which roughly corresponds to the fraction of variance in the data explained by the model.\n",
    "\n",
    "-   Let us calculate the R^2 value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-0.0006, -0.0004, -0.0011, -0.0008, -0.0006,  0.0009,  0.0025,\n",
      "        0.0025, -0.0024])\n",
      "R-squared = 0.9999972914903201 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "import pprint\n",
    "\n",
    "x = np.array([0, 0.5, 1, 1.5, 2.0, 3.0, 4.0, 6.0, 10])\n",
    "y = np.array([0, -0.157, -0.315, -0.472, -0.629, -0.942, -1.255, -1.884, -3.147])\n",
    "\n",
    "X = np.column_stack([x, x**0])\n",
    "\n",
    "#  I find these intermediate variables make it easier to read\n",
    "XTX = np.dot(X.T, X)\n",
    "XTy = np.dot(X.T, y)\n",
    "\n",
    "p = np.dot(np.linalg.inv(XTX), XTy)\n",
    "\n",
    "e = y - np.dot(X,p)\n",
    "pprint.pprint(e)\n",
    "\n",
    "SSE = np.dot(e, e)\n",
    "\n",
    "yb = y - np.mean(y)\n",
    "SST = np.dot(yb, yb)\n",
    "Rsq = 1 - SSE/SST\n",
    "\n",
    "print('R-squared = {0} '.format(Rsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The R^2 tells you how much of the variation in the data is explained by the model.\n",
    "    -   a value of 1 tells you all the variation is explained\n",
    "    -   values less than one means the model is incomplete in some way\n",
    "    -   Here the value is close to one, which suggests a good fit\n",
    "\n",
    "-   It is important to consider the uncertainty on the parameters\n",
    "\n",
    "-   pycse has a `regress` function for that\n",
    "    -   We specify a confidence level, typically 95%\n",
    "    -   &alpha; = (100 - %confidence level)/100\n",
    "    -   Let us apply that to the same data set\n",
    "\n",
    "[pycse.regress](https://www.google.com/#safe=off&q=pycse.regress)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slope is between [-0.315 -0.314] \n",
      "at the 95% confidence level\n",
      "The intercept is between [-0.0014  0.0027] \n",
      "at the 95% confidence level\n"
     ]
    }
   ],
   "source": [
    "from pycse import regress\n",
    "import numpy as np\n",
    "x = np.array([0, 0.5, 1, 1.5, 2.0, 3.0, 4.0, 6.0, 10])\n",
    "y = np.array([0, -0.157, -0.315, -0.472, -0.629, -0.942, -1.255, -1.884, -3.147])\n",
    "\n",
    "X = np.column_stack([x, x**0])\n",
    "\n",
    "# Choose 95% confidence level\n",
    "alpha = 1 - 0.95\n",
    "p, pint, se = regress(X, y, alpha)\n",
    "slope_interval, intercept_interval = pint\n",
    "\n",
    "print('The slope is between {0} \\n'\n",
    "      'at the 95% confidence level'.format(slope_interval))\n",
    "\n",
    "print('The intercept is between {0} \\n'\n",
    "      'at the 95% confidence level'.format(intercept_interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Note in this case that the second parameter includes 0\n",
    "    -   We cannot conclude that this parameter is significant.\n",
    "    -   A simpler model with the intercept fixed at 0 might be better\n",
    "\n",
    "-   The size of the confidence intervals depends on the number of data points, the number of estimated parameters, and the confidence level.\n",
    "\n",
    "Read the [regress](https://github.com/jkitchin/pycse/blob/master/pycse/PYCSE.py#L7) source code to learn how the confidence intervals are calculated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applications in determining a rate constant and reaction order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Rate constants and reaction orders are determined by using models that are fit to experimental data\n",
    "\n",
    "-   A common case is to monitor concentration vs. time in a constant volume, batch reactor\n",
    "\n",
    "-   We consider the disappearance of $A$\n",
    "\n",
    "-   From the mole balance we know:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dN_A}{dt} = r_A V\n",
    "\\end{equation}\n",
    "\n",
    "-   Let us assume the rate law is of the form: $r_A = k C_A^\\alpha$ and a constant volume so that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dC_A}{dt} = -k C_A^\\alpha\n",
    "\\end{equation}\n",
    "\n",
    "-   Let us be loose with mathematics, rearrange the equation, and take the log of both sides.\n",
    "    -   By loose I mean we take logs of quantities that are not dimensionless\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln(-\\frac{dC_A}{dt}) = \\ln{k} + \\alpha \\ln C_A\n",
    "\\end{equation}\n",
    "\n",
    "-   This suggests that if we could numerically compute $\\frac{dC_A}{dt}$ from our data of $C_A(t)$ then a plot of the log of the negative derivative vs the log of concentration would have\n",
    "    -   an intercept equal to the log of the rate constant, $k$\n",
    "    -   and a slope equal to the reaction order $\\alpha$\n",
    "\n",
    "-   Given the following data, determine the reaction order in A and the rate constant with 95% confidence intervals.\n",
    "\n",
    "<table id=\"orgfd5ae74\" border=\"2\" cellspacing=\"0\" cellpadding=\"6\" rules=\"groups\" frame=\"hsides\">\n",
    "\n",
    "\n",
    "<colgroup>\n",
    "<col  class=\"org-right\" />\n",
    "\n",
    "<col  class=\"org-right\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr>\n",
    "<th scope=\"col\" class=\"org-right\">time (min)</th>\n",
    "<th scope=\"col\" class=\"org-right\">C\\_A (mol/L)</th>\n",
    "</tr>\n",
    "</thead>\n",
    "\n",
    "<tbody>\n",
    "<tr>\n",
    "<td class=\"org-right\">0</td>\n",
    "<td class=\"org-right\">0.0500</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td class=\"org-right\">50</td>\n",
    "<td class=\"org-right\">0.0380</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td class=\"org-right\">100</td>\n",
    "<td class=\"org-right\">0.0306</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td class=\"org-right\">150</td>\n",
    "<td class=\"org-right\">0.0256</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td class=\"org-right\">200</td>\n",
    "<td class=\"org-right\">0.0222</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td class=\"org-right\">250</td>\n",
    "<td class=\"org-right\">0.0195</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td class=\"org-right\">300</td>\n",
    "<td class=\"org-right\">0.0174</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "-   We will use the `pycse.deriv` function to numerically compute centered 2-point finite difference approximations to the derivatives\n",
    "-   This works best when the $x$ points are evenly spaced, and they should be monotically increasing or decreasing\n",
    "\n",
    "[pycse.deriv](https://www.google.com/#safe=off&q=pycse.deriv)\n",
    "\n",
    "Read the [deriv](https://github.com/jkitchin/pycse/blob/master/pycse/PYCSE.py#L182) source code to learn how the derivatives are approximated, and what options are available.\n",
    "\n",
    "-   Note that we are actually using the data in table [tab-data>](tab-data>)in this code block!\n",
    "\n",
    "-   We do not have to type the data in ourselves.\n",
    "\n",
    "-   This causes some false reporting in pyflakes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9fdc4eea1bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to convert the list of numbers to a numpy array so we can do the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-41d92eff1f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# data will be a 2d list, which we convert to an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# column 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mCa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# column 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)  # alternate approach to printing accuracy\n",
    "from pycse import deriv, regress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data will be a 2d list, which we convert to an array here\n",
    "data = np.array(data)\n",
    "t = data[:, 0]   # column 0\n",
    "Ca = data[:, 1]  # column 1\n",
    "\n",
    "# calculate numerical derivatives\n",
    "dCadt = deriv(t, Ca)\n",
    "\n",
    "# do the transformation\n",
    "x = np.log(Ca)\n",
    "y = np.log(-dCadt)\n",
    "\n",
    "# setup and do the regression\n",
    "# column of ones and x:  y = b + mx\n",
    "X = np.column_stack([x**0, x])\n",
    "\n",
    "p, pint, se = regress(X, y, 0.05)\n",
    "\n",
    "intercept_range = pint[0]\n",
    "alpha_range = pint[1]\n",
    "\n",
    "k = np.exp(intercept_range)\n",
    "\n",
    "print('alpha = {0} at the 95% confidence level'.format(alpha_range))\n",
    "print('k = {0} at the 95% confidence level'.format(k))\n",
    "\n",
    "# always visually inspect the fit\n",
    "plt.plot(x, y,'ko ')\n",
    "plt.plot(x, np.dot(X, p))\n",
    "plt.xlabel('$\\ln(C_A)$')\n",
    "plt.ylabel('$\\ln(-dC_A/dt)$')\n",
    "plt.savefig('images/regression-rate.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./images/regression-rate.png)\n",
    "\n",
    "-   You can see there is a reasonably large range of values for the rate constant and reaction order (although the confidence interval does not contain zero)\n",
    "\n",
    "-   The fit looks ok, but you can see the errors are not exactly random\n",
    "    -   There seems to be systematic trends in a sigmoidal shape of the data\n",
    "    -   That suggests small inadequacy in the model\n",
    "\n",
    "-   Let us examine some methods of evaluating the quality of fit\n",
    "\n",
    "-   First we examine the residuals, or the errors between the data and the model.\n",
    "\n",
    "-   In a good fit, these will be randomly distributed\n",
    "\n",
    "-   In a less good fit, there will be trends\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "from pycse import deriv, regress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data will be a 2d list, which we convert to an array here\n",
    "data = np.array(data)\n",
    "t = data[:, 0]\n",
    "Ca = data[:, 1]\n",
    "\n",
    "# calculate numerical derivatives\n",
    "dCadt = deriv(t, Ca)\n",
    "\n",
    "# do the transformation\n",
    "x = np.log(Ca)\n",
    "y = np.log(-dCadt)\n",
    "\n",
    "# setup and do the regression\n",
    "# column of ones and x:  y = b + mx\n",
    "X = np.column_stack([x**0, x])\n",
    "\n",
    "p, pint, se = regress(X, y, 0.05)\n",
    "\n",
    "residuals = y - np.dot(X, p)\n",
    "\n",
    "# always visually inspect the fit\n",
    "plt.plot(x, residuals, 'ko-')\n",
    "plt.xlabel('$\\ln(C_A)$')\n",
    "plt.ylabel('residuals')\n",
    "plt.savefig('images/regression-residuals.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./images/regression-residuals.png)\n",
    "\n",
    "-   You can see there are trends in this data\n",
    "    -   That means the model may not be complete\n",
    "\n",
    "-   There is uncertainty in the data\n",
    "    -   In each concentration measurement there is uncertainty in the time and value of concentration\n",
    "    -   You need more data to reduce the uncertainty\n",
    "    -   You may also need better data to reduce the uncertainty\n",
    "\n",
    "-   Derivatives tend to *magnify* errors in data\n",
    "    -   The method we used to fit the data contributed to the uncertainty\n",
    "\n",
    "-   We also *nonlinearly* transformed the errors by taking logs and exp of the data and results, which may have skewed the confidence limits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hybrid methods for data analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Numerical differentiation is noisy, but does the least amount of data manipulation, e.g. smoothing\n",
    "\n",
    "-   Let us consider some hybrid approaches\n",
    "\n",
    "-   The first hybrid method is to fit a polynomial to the Ca(t) data, and then analytically differentiate the polynomial\n",
    "\n",
    "-   You must use some judgment about what order polynomial to fit\n",
    "    -   Judgment comes from experience\n",
    "\n",
    "[numpy.polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)  Fit a polynomial to data\n",
    "\n",
    "[numpy.polyder](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyder.html)  Get the derivative of a polynomial\n",
    "\n",
    "[numpy.polyval](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyval.html)  Evaluate a polynomial at some data points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "from pycse import regress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data will be a 2d list, which we convert to an array here\n",
    "data = np.array(data)\n",
    "t = data[:, 0]\n",
    "Ca = data[:, 1]\n",
    "\n",
    "pCa = np.polyfit(t, Ca, 4)\n",
    "\n",
    "fCa = np.polyval(pCa, t)\n",
    "\n",
    "print('Summed squared error = {}'.format(sum(fCa - Ca)**2))\n",
    "\n",
    "# always visually inspect the fit\n",
    "plt.plot(t, Ca, 'ko ')\n",
    "plt.plot(t, fCa)\n",
    "plt.xlabel('$t$ (min)')\n",
    "plt.ylabel('$C_A$ (mol/L)')\n",
    "plt.title('Polynomial fit to the data')\n",
    "plt.savefig('images/polyfit-1.png')\n",
    "\n",
    "# [[./images/polyfit-1.png]]\n",
    "\n",
    "# get the derivative\n",
    "dCadt = np.polyval(np.polyder(pCa), t)\n",
    "\n",
    "# Construct the data we want to fit\n",
    "# ln(-dCa/dt) = alpha ln(Ca) + ln(k)\n",
    "x = np.log(Ca)\n",
    "y = np.log(-dCadt)\n",
    "\n",
    "X = np.column_stack([x**0, x])\n",
    "p, pint, se = regress(X, y, 0.05)\n",
    "\n",
    "intercept_range = pint[0]\n",
    "alpha_range = pint[1]\n",
    "\n",
    "k = np.exp(intercept_range)\n",
    "\n",
    "print('alpha = {} at the 95% confidence level'.format(alpha_range))\n",
    "print('k = {0} at the 95% confidence level'.format(k))\n",
    "\n",
    "# always visually inspect the fit\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'ko ')\n",
    "plt.plot(x, np.dot(X, p))\n",
    "plt.xlabel('$\\ln(C_A)$')\n",
    "plt.ylabel('$\\ln(-dC_A/dt)$')\n",
    "plt.savefig('images/poly-regression-rate.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./images/poly-regression-rate.png)\n",
    "\n",
    "-   Note the confidence intervals are tighter\n",
    "-   That is because the polynomial fitting smooths some of the errors out\n",
    "-   We still have nonlinearly transformed errors which may skew the confidence intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Nonlinear regression review\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Nonlinear models are abundant in reaction engineering\n",
    "    -   $r = k C_A^n $ is linear in the $k$ parameter, and nonlinear in $n$\n",
    "\n",
    "-   Nonlinear fitting is essentially a non-linear optimization problem\n",
    "\n",
    "-   Unlike linear regression, where we directly compute the parameters using matrix algebra, we have to provide an initial guess and iterate to the solution\n",
    "\n",
    "-   Similar to using fsolve, we must define a function of the model\n",
    "    -   The function takes an independent variable, and parameters, f(x,a,b,&#x2026;)\n",
    "    -   The function should return a value of $y$ for every value of $x$\n",
    "    -   i.e. it should be vectorized\n",
    "\n",
    "-   It is possible to formulate these problems as nonlinear minimization of summed squared errors. See [this example](http://jkitchin.github.io/blog/2013/02/18/Nonlinear-curve-fitting/).\n",
    "\n",
    "-   The function `scipy.optimize.curve_fit` provides nonlinear fitting of models (functions) to data.\n",
    "\n",
    "[scipy.optimize.curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html)\n",
    "\n",
    "-   Here is an example usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "x = np.array([0.5, 0.387, 0.24, 0.136, 0.04, 0.011])\n",
    "y = np.array([1.255, 1.25, 1.189, 1.124, 0.783, 0.402])\n",
    "\n",
    "# this is the function we want to fit to our data\n",
    "def func(x, a, b):\n",
    "    'nonlinear function in a and b to fit to data'\n",
    "    return a * x / (b + x)\n",
    "\n",
    "initial_guess = [1.2, 0.03]\n",
    "\n",
    "pars, pcov = curve_fit(func, x, y, p0=initial_guess)\n",
    "\n",
    "a,b = pars\n",
    "print('a = {0} and b={1}'.format(a,b))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,y,'bo ')\n",
    "xfit = np.linspace(min(x), max(x))\n",
    "yfit = func(xfit, *pars)\n",
    "plt.plot(xfit,yfit,'b-')\n",
    "plt.legend(['data','fit'],loc='best')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig('images/nonlin-curve-fit.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./images/nonlin-curve-fit.png)\n",
    "\n",
    "-   Again, you should always visually inspect the fit\n",
    "\n",
    "Practice: Repeat this last example by creating a function that calculates the summed squared errors between a model function and the data. Use fmin to find the parameters that minimizes the summed squared error.\n",
    "\n",
    "-   We also need to estimate uncertainties in nonlinear parameters\n",
    "\n",
    "-   `pycse` provides a function for this: `nlinfit`.\n",
    "\n",
    "[pycse.nlinfit](https://www.google.com/#safe=off&q=pycse.nlinfit)\n",
    "\n",
    "Read the [nlinfit](https://github.com/jkitchin/pycse/blob/master/pycse/PYCSE.py#L53) source code to see how the confidence intervals are computed\n",
    "\n",
    "Here is an example usage of nlinfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "from pycse import nlinfit\n",
    "\n",
    "x = np.array([0.5, 0.387, 0.24, 0.136, 0.04, 0.011])\n",
    "y = np.array([1.255, 1.25, 1.189, 1.124, 0.783, 0.402])\n",
    "\n",
    "\n",
    "def func(x, a, b):\n",
    "    'nonlinear function in a and b to fit to data'\n",
    "    return a * x / (b + x)\n",
    "\n",
    "initial_guess = [1.2, 0.03]\n",
    "alpha = 0.05\n",
    "pars, pint, se = nlinfit(func, x, y, initial_guess, alpha)\n",
    "\n",
    "aint, bint = np.array(pint)\n",
    "print('The 95% confidence interval on a is {0}'.format(aint))\n",
    "print('The 95% confidence interval on b is {0}'.format(bint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Here the two intervals are relatively small, and do not include zero, suggesting both parameters are significant.\n",
    "\n",
    "-   More importantly, the errors are not skewed by a nonlinear transformation.\n",
    "\n",
    "-   Note you have to provide an initial guess.\n",
    "    -   This will not always be easy to guess.\n",
    "    -   There may be more than one minimum in the fit also, so different guesses may give different parameters.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
